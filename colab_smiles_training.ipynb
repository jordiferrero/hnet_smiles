{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HNet SMILES on Google Colab\n",
        "\n",
        "This notebook installs all dependencies on a Colab GPU runtime and runs training/generation for the SMILES dataset in this repository.\n",
        "\n",
        "Requirements:\n",
        "\n",
        "- Colab runtime set to GPU (Runtime -> Change runtime type -> Hardware accelerator: GPU)\n",
        "- Internet access (to install dependencies and optionally clone the repo)\n",
        "\n",
        "Tips:\n",
        "\n",
        "- You can save checkpoints and outputs to Google Drive by mounting it (optional cell included).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU runtime\n",
        "import torch, platform\n",
        "\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Please enable GPU in Runtime -> Change runtime type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: mount Google Drive to persist checkpoints\n",
        "USE_DRIVE = False  # set to True to enable\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "    import os\n",
        "\n",
        "    os.system(\"mkdir -p /content/drive/MyDrive/hnet_smiles\")\n",
        "    WORKDIR = \"/content/drive/MyDrive/hnet_smiles\"\n",
        "else:\n",
        "    WORKDIR = \"/content\"\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"WORKDIR\"] = WORKDIR\n",
        "print(\"Working directory:\", WORKDIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the repo into the Colab runtime\n",
        "import os, subprocess\n",
        "\n",
        "repo_url = \"https://github.com/jordiferrero/hnet_smiles\"\n",
        "workdir = os.environ.get(\"WORKDIR\", \"/content\")\n",
        "target_dir = os.path.join(workdir, \"hnet_smiles\")\n",
        "if not os.path.isdir(os.path.join(target_dir, \".git\")):\n",
        "    subprocess.run([\"git\", \"clone\", repo_url, target_dir], check=True)\n",
        "else:\n",
        "    subprocess.run(\n",
        "        [\"bash\", \"-lc\", f'cd \"{target_dir}\" && git pull --ff-only || true'], check=False\n",
        "    )\n",
        "os.chdir(target_dir)\n",
        "print(\"CWD:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core Python deps and CUDA-specific libs\n",
        "import sys, subprocess\n",
        "\n",
        "# Upgrade pip\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"], check=True)\n",
        "\n",
        "# Core deps\n",
        "subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"setup/requirements.txt\"],\n",
        "    check=False,\n",
        ")\n",
        "\n",
        "# CUDA-specific libs (Colab has CUDA; these may take a while). If wheels exist, they will be used.\n",
        "# FlashAttention\n",
        "subprocess.run(\n",
        "    [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"pip\",\n",
        "        \"install\",\n",
        "        \"flash-attn==2.6.3\",\n",
        "        \"--no-build-isolation\",\n",
        "    ],\n",
        "    check=False,\n",
        ")\n",
        "\n",
        "# mamba_ssm and causal-conv1d pinned to known SHAs\n",
        "subprocess.run(\n",
        "    [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"pip\",\n",
        "        \"install\",\n",
        "        \"git+https://github.com/state-spaces/mamba.git@a6a1dae6efbf804c9944a0c2282b437deb4886d8\",\n",
        "    ],\n",
        "    check=False,\n",
        ")\n",
        "subprocess.run(\n",
        "    [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"pip\",\n",
        "        \"install\",\n",
        "        \"git+https://github.com/Dao-AILab/causal-conv1d.git@e940ead2fd962c56854455017541384909ca669f\",\n",
        "    ],\n",
        "    check=False,\n",
        ")\n",
        "\n",
        "# Install local hnet package last (allow resolving deps already installed)\n",
        "subprocess.run(\n",
        "    [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"pip\",\n",
        "        \"install\",\n",
        "        \"-e\",\n",
        "        \"original_resources/hnet-github-repo\",\n",
        "        \"--no-deps\",\n",
        "    ],\n",
        "    check=True,\n",
        ")\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) Quick data analysis plot\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "!python data/analyze_smiles.py --csv-path datasets/PI1M/PI1M_v2.csv --plot --output-dir visualizations\n",
        "\n",
        "stats_path = Path('visualizations/smiles_statistics.json')\n",
        "if stats_path.exists():\n",
        "    with open(stats_path) as f:\n",
        "        stats = json.load(f)\n",
        "    print({k: stats['length_stats'][k] for k in ['mean','median','q90','q95']})\n",
        "else:\n",
        "    print('Stats file not found (skipping)')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train: Small phase (1K samples)\n",
        "!python train_smiles.py \\\n",
        "  --config configs/hnet_smiles_small.json \\\n",
        "  --phase small \\\n",
        "  --max-samples 1000 \\\n",
        "  --batch-size 8 \\\n",
        "  --epochs 5 \\\n",
        "  --output-dir checkpoints\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a few tokens from the trained small-phase checkpoint\n",
        "from pathlib import Path\n",
        "import subprocess, sys\n",
        "\n",
        "ckpt_candidates = (\n",
        "    sorted(Path(\"checkpoints\").glob(\"checkpoint_phase_small_epoch_*.pt\"))\n",
        "    if Path(\"checkpoints\").exists()\n",
        "    else []\n",
        ")\n",
        "ckpt = ckpt_candidates[-1] if ckpt_candidates else None\n",
        "print(\"Using checkpoint:\", ckpt)\n",
        "\n",
        "if ckpt is not None:\n",
        "    subprocess.run(\n",
        "        [\n",
        "            sys.executable,\n",
        "            \"generate_smiles.py\",\n",
        "            \"--checkpoint\",\n",
        "            str(ckpt),\n",
        "            \"--config\",\n",
        "            \"configs/hnet_smiles_small.json\",\n",
        "            \"--prompt\",\n",
        "            \"*\",\n",
        "            \"--max-tokens\",\n",
        "            \"256\",\n",
        "            \"--temperature\",\n",
        "            \"1.0\",\n",
        "        ],\n",
        "        check=True,\n",
        "    )\n",
        "else:\n",
        "    print(\"No checkpoint found; run the training cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize dynamic chunking (creates an animated GIF)\n",
        "from pathlib import Path\n",
        "import subprocess, sys\n",
        "\n",
        "ckpt_candidates = (\n",
        "    sorted(Path(\"checkpoints\").glob(\"checkpoint_phase_small_epoch_*.pt\"))\n",
        "    if Path(\"checkpoints\").exists()\n",
        "    else []\n",
        ")\n",
        "ckpt = ckpt_candidates[-1] if ckpt_candidates else None\n",
        "print(\"Using checkpoint:\", ckpt)\n",
        "\n",
        "out_dir = Path(\"visualizations/output\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"chunking_example.gif\"\n",
        "\n",
        "if ckpt is not None:\n",
        "    subprocess.run(\n",
        "        [\n",
        "            sys.executable,\n",
        "            \"visualizations/visualize_chunking.py\",\n",
        "            \"--checkpoint\",\n",
        "            str(ckpt),\n",
        "            \"--config\",\n",
        "            \"configs/hnet_smiles_small.json\",\n",
        "            \"--text\",\n",
        "            \"*CCC[Fe]CCCC(=O)OCCCCOCCCNCC(*)=O\",\n",
        "            \"--output\",\n",
        "            str(out_path),\n",
        "        ],\n",
        "        check=True,\n",
        "    )\n",
        "    print(\"Saved:\", out_path)\n",
        "else:\n",
        "    print(\"No checkpoint found; run the training cell first.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
